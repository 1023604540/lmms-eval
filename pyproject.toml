[build-system]
requires = ["setuptools>=40.8.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "lmm_eval"
version = "0.1.0"
authors = [
    {name="Peiyuan Zhang, Bo Li, Yi Fan", email="a1286225768@gmail.com"}
]
description = "A framework for evaluating large multi-modality language models"
readme = "README.md"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
requires-python = ">=3.8"
license = { "text" = "MIT" }
dependencies = [
    "accelerate>=0.21.0",
    "evaluate",
    "datasets>=2.0.0",
    "evaluate>=0.4.0",
    "jsonlines",
    "numexpr",
    "peft>=0.2.0",
    "pybind11>=2.6.2",
    "pytablewriter",
    "rouge-score>=0.0.4",
    "sacrebleu>=1.5.0",
    "scikit-learn>=0.24.1",
    "sqlitedict",
    "torch>=1.8",
    "tqdm-multiprocess",
    "transformers>=4.1",
    "llava @ git+https://github.com/haotian-liu/LLaVA",
    "zstandard",
]

[tool.setuptools.packages.find]
include = ["lmm_eval*"]

# required to include yaml files in pip installation
[tool.setuptools.package-data]
lmm_eval = ["**/*.yaml", "tasks/**/*"]

[project.scripts]
lmm-eval = "lmm_eval.__main__:cli_evaluate"
lmm_eval = "lmm_eval.__main__:cli_evaluate"

[project.urls]
Homepage = "Null"
Repository = "Null"

[project.optional-dependencies]
dev = ["black", "flake8", "pre-commit", "pytest", "pytest-cov"]
linting = [
    "flake8",
    "pylint",
    "mypy",
    "pre-commit",
]
testing = ["pytest", "pytest-cov", "pytest-xdist"]
multilingual = ["nagisa>=0.2.7", "jieba>=0.42.1", "pycountry"]
math = ["sympy>=1.12", "antlr4-python3-runtime==4.11"]
sentencepiece = ["sentencepiece>=0.1.98", "protobuf>=4.22.1"]
gptq = ["auto-gptq[triton] @ git+https://github.com/PanQiWei/AutoGPTQ"]
anthropic = ["anthropic"]
openai = ["openai==1.3.9", "tiktoken"]
vllm = ["vllm"]
ifeval = ["langdetect", "immutabledict"]
all = [
    "lmm_eval[dev]",
    "lmm_eval[testing]",
    "lmm_eval[linting]",
    "lmm_eval[multilingual]",
    "lmm_eval[sentencepiece]",
    "lmm_eval[promptsource]",
    "lmm_eval[gptq]",
    "lmm_eval[anthropic]",
    "lmm_eval[openai]",
    "lmm_eval[vllm]",
    "lmm_eval[ifeval]",
]
